{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406d1cfb-72ad-4eb8-bb10-c58664436ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOvde je ideja da koristimo Neuralnu Mrezu koju smo istrenirali  na prerecorded videima d\\na je testiramo sa videa koji dolazi live sa webcamere\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Ovde je ideja da koristimo Neuralnu Mrezu koju smo istrenirali  na prerecorded videima d\n",
    "a je testiramo sa videa koji dolazi live sa webcamere\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7954b791-b7cf-4bea-834f-bb70bac7a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import cv2\n",
    "import dlib\n",
    "import imageio\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "from threading import Thread\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad76d62-1ab5-4602-a13e-65105d58ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, Dense, GRU, Dropout, Bidirectional, MaxPool3D\n",
    "from tensorflow.keras.layers import Flatten, Activation, TimeDistributed \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.initializers import Orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85e4b85-6492-4098-b8e7-8193ebb58cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#varijable\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "global is_talking #upravo se desava govor\n",
    "global last_talking_time #poslednji put kada je govor bio detektovan (u sekundama)\n",
    "global flag_record #flag se aktivira ako su usta otvorena ili ako je proslo < 0.5 sekundi od zatvaranja usta(verovatno izgovaranje m ili p slova)\n",
    "global prev_flag_record #da bi detektovali prelaz flag_record sa 1 na 0 i znamo da je gotova sekvenca pricanja i mozemo da obradimo podatke(funkcija procces_webcam)\n",
    "global done #detektuje da li je izvrsena predikcija, ukoliko je 0 ne uzima nove frejmove sa kamere dok neuralna mreza ne uradi svoje\n",
    "\n",
    "flag_record = False \n",
    "is_talking = False\n",
    "last_talking_time = 0\n",
    "prev_flag_record = None\n",
    "done = 1 #prvobitno 1 da bi se detektovao prvi govor (funckija process_webcam i predict_nn)\n",
    "\n",
    "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdda1325-4c1b-4f30-8f7a-82efdcd24a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(128, 3, input_shape=(75, 64, 64, 1), padding='same')) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1, 2, 2)))\n",
    "#print(model.output_shape)\n",
    "\n",
    "model.add(Conv3D(256, 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1, 2, 2)))\n",
    "#print(model.output_shape)\n",
    "\n",
    "model.add(Conv3D(64, 3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1, 2, 2)))\n",
    "#print(model.output_shape)\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Bidirectional(GRU(128, kernel_initializer=Orthogonal(), return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Bidirectional(GRU(128, kernel_initializer=Orthogonal(), return_sequences=True)))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "#kernel_initializer='he_normal' inicijalizujemo tezine(weights) da odrze kvalitetan gradijent pri obuci\n",
    "model.add(Dense(char_to_num.vocabulary_size() + 1, kernel_initializer='he_normal', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "747e0e36-675d-485d-b789-208474b5c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./Weights/weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf196244-dd2b-4f76-b38c-05859b2ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Funkcija koja procesuira frejm kada je detektovan govor na web-kameri \n",
    "i vraća tensor sa frejmovima govora, pripremljen za predikciju u neuronskoj mreži\n",
    "'''\n",
    "\n",
    "def process_frame(detector, predictor, frame, width: int = 64, height: int = 64) -> List[float]: \n",
    "    \n",
    "    global flag_record\n",
    "    if flag_record:\n",
    "        frames = []\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray_frame)\n",
    "        lip_coords = None\n",
    "    \n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray_frame, face)\n",
    "            \n",
    "            lip_left = landmarks.part(48).x\n",
    "            lip_right = landmarks.part(54).x\n",
    "            lip_top = min(landmarks.part(50).y, landmarks.part(51).y)\n",
    "            lip_bottom = max(landmarks.part(58).y, landmarks.part(59).y)\n",
    "    \n",
    "            lip_frame = frame[lip_top:lip_bottom, lip_left:lip_right]\n",
    "            lip_frame_resized = cv2.resize(lip_frame, (width, height))  # Define `width` and `height`\n",
    "            lip_frame_gray = cv2.cvtColor(lip_frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "            frames.append(lip_frame_gray)\n",
    "            \n",
    "            # Update lip_coords with actual coordinates if needed\n",
    "            lip_coords = (lip_left, lip_right, lip_top, lip_bottom)\n",
    "    \n",
    "        if frames:\n",
    "            frames = tf.convert_to_tensor(frames, dtype=tf.float32)\n",
    "            mean = tf.reduce_mean(frames)\n",
    "            std = tf.math.reduce_std(frames)\n",
    "            frames = tf.cast((frames - mean), tf.float32) / std\n",
    "\n",
    "            frames = tf.expand_dims(frames, axis=-1)  # da input bude dobar za ulaz mreze\n",
    "            current_frame_count = tf.shape(frames)[0]\n",
    "            if current_frame_count < 75:\n",
    "                padding_frames = 75 - current_frame_count\n",
    "                frames = tf.pad(frames, [[0, padding_frames], [0, 0], [0, 0], [0, 0]])\n",
    "                frames = tf.expand_dims(frames, axis=0)\n",
    "                print(frames.shape)\n",
    "            if current_frame_count >= 75:\n",
    "                frames = tf.pad(frames, [[0,75], [0, 0], [0, 0], [0, 0]])\n",
    "                frames = tf.expand_dims(frames, axis=0)\n",
    "                print(frames.shape)\n",
    "            \n",
    "            return frames, lip_coords\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "435f9a05-a9a6-4e3c-96c1-54babd768082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizuelna reprezentacija na web-kameri koja pokazuje da je algoritam uspešno detektovao lice i izdvojio regiju usta\n",
    "\n",
    "def process_frame_overlay(detector, predictor,  frame, width: int = 64, height: int = 64) -> np.ndarray: \n",
    "    \n",
    "    global flag_record, done\n",
    "    global is_talking, last_talking_time\n",
    "    current_time = time.time()\n",
    "        \n",
    "    frames = []\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray_frame)\n",
    "\n",
    "    if done == 0: #ukoliko se prediktuje ne radi nista dok se to ne zavrsi\n",
    "        cv2.putText(frame, \"Processing...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (50, 255, 50), 2)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No Face detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    else:\n",
    "        \n",
    "        for face in faces:\n",
    "            \n",
    "            landmarks = predictor(gray_frame, face)\n",
    "            \n",
    "            lip_left = landmarks.part(48).x\n",
    "            lip_right = landmarks.part(54).x\n",
    "            lip_top = min(landmarks.part(50).y, landmarks.part(51).y)\n",
    "            lip_bottom = max(landmarks.part(58).y, landmarks.part(59).y)\n",
    "    \n",
    "            #print(f\"Lip coordinates: left={lip_left}, right={lip_right}, top={lip_top}, bottom={lip_bottom}\")\n",
    "            mouth_top = (landmarks.part(51).x, landmarks.part(51).y)\n",
    "            mouth_bottom = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "            lip_distance = math.hypot(mouth_bottom[0] - mouth_top[0], mouth_bottom[1] - mouth_top[1])\n",
    "            \n",
    "            lip_frame = frame[lip_top:lip_bottom, lip_left:lip_right]\n",
    "            lip_frame_resized = cv2.resize(lip_frame, (width, height))\n",
    "            lip_frame_gray = cv2.cvtColor(lip_frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "                 \n",
    "            frames.append(lip_frame_gray)\n",
    "            #print(lip_distance)\n",
    "            if lip_distance >= 19.5:\n",
    "                \n",
    "                flag_record = True\n",
    "                is_talking = True\n",
    "               \n",
    "                last_talking_time = current_time\n",
    "                \n",
    "                cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                #for n in range(48, 61):\n",
    "                #    x = landmarks.part(n).x\n",
    "                #    y = landmarks.part(n).y\n",
    "                #    cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if is_talking and (current_time - last_talking_time >= 0.5):\n",
    "\n",
    "                    flag_record = False\n",
    "                    is_talking = False\n",
    "                    \n",
    "                    cv2.putText(frame, \"Ready\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    #for n in range(48, 61):\n",
    "                    #    x = landmarks.part(n).x\n",
    "                    #    y = landmarks.part(n).y\n",
    "                    #    cv2.circle(img=frame, center=(x, y), radius=3, color=(255, 0, 0), thickness=-1)\n",
    "                else:\n",
    "                    if is_talking:\n",
    "\n",
    "                        flag_record = True\n",
    "                        \n",
    "                        cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        #for n in range(48, 61):\n",
    "                        #    x = landmarks.part(n).x\n",
    "                        #    y = landmarks.part(n).y\n",
    "                        #    cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "                    else:\n",
    "\n",
    "                        flag_record = False\n",
    "                        \n",
    "                        cv2.putText(frame, \"Ready\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                        #for n in range(48, 61):\n",
    "                        #    x = landmarks.part(n).x\n",
    "                        #    y = landmarks.part(n).y\n",
    "                        #    cv2.circle(img=frame, center=(x, y), radius=3, color=(255, 0, 0), thickness=-1)\n",
    "   \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a395e4c9-9a38-4afb-87e8-c9b7bb255fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "funkcija koja snima deo u kojem je detektovan govor i čuva ga kao GIF, \n",
    "pružajući vizuelnu reprezentaciju podataka koji će biti korišćeni u neuronskoj mreži.\n",
    "'''\n",
    "\n",
    "def save_talking_as_gif(collected_frames):\n",
    "    \n",
    "        frames_array = np.array(collected_frames)\n",
    "        frames_array_uint8 = np.uint8(frames_array * 255)\n",
    "        frames_array_uint8_squeezed = np.squeeze(frames_array_uint8)\n",
    "        imageio.mimsave('./animation1.gif', frames_array_uint8_squeezed, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a6b665a-7247-46e8-b116-555255edb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn(dataset):\n",
    "\n",
    "    global done, output\n",
    "   \n",
    "    frames = [frame.numpy() for frame in dataset]\n",
    "    frames = tf.stack(frames)  \n",
    "\n",
    "    yhat = model.predict(frames)\n",
    "    output = tf.strings.reduce_join([num_to_char(tf.argmax(x)) for x in yhat[0]])\n",
    "\n",
    "    print(output)\n",
    "    done = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16faace0-d9c3-46ae-8b92-17a404720559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while loop\n",
    "\n",
    "def process_webcam():\n",
    "    \n",
    "    global prev_flag_record, done, output\n",
    "    output_str = ''\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    collected_frames = []\n",
    "   \n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "\n",
    "        flipped_frame = cv2.flip(frame, 1)\n",
    "        lip_frames, _ = process_frame(detector, predictor, flipped_frame) \n",
    "        processed_frame_overlay = process_frame_overlay(detector, predictor, flipped_frame) \n",
    "        \n",
    "        if lip_frames is not None: \n",
    "            collected_frames.extend(lip_frames)\n",
    "\n",
    "        if prev_flag_record == 1 and flag_record == 0 and done  == 1:  #detektujemo prelaz sa 1 na 0 (tj prestanak govora i ponovo cutanje i znamo da mozemo da obradimo taj snimak)\n",
    "                if collected_frames:\n",
    "                    done = 0\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices(collected_frames)\n",
    "                    #save_talking_as_gif(dataset) #vrati animation1.gif gde je snimljena sekvenca govora\n",
    "                    t1 = Thread(target = predict_nn, args = (dataset,))\n",
    "                    t1.start()\n",
    "                    collected_frames = [] #nakon predikcije teksta cistimo listu gde je bio govor za sledeci govor    \n",
    "        \n",
    "    \n",
    "            \n",
    "        prev_flag_record = flag_record \n",
    "        cv2.imshow('Webcam Overlay', processed_frame_overlay)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66e999-6dfb-4d16-b1df-07adb96d8f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 75, 64, 64, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "tf.Tensor(b\"''aaaaaaaa6666666bb99999999zzeeeeeaa\", shape=(), dtype=string)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n",
      "(1, 75, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "process_webcam()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5eec1-7bd9-4f59-9324-eaa1ba272239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51b499-c054-4e9e-9396-b7406d597cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
