{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406d1cfb-72ad-4eb8-bb10-c58664436ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ovde je ideja da koristimo Neuralnu Mrezu koju smo istrenirali na prerecorded videima\n",
    "#da je testiramo sa videa koji dolazi live sa webcamere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7954b791-b7cf-4bea-834f-bb70bac7a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import cv2\n",
    "import dlib\n",
    "import imageio\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85e4b85-6492-4098-b8e7-8193ebb58cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#varijable\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "global is_talking #upravo se desava govor\n",
    "global last_talking_time #poslednji put kada je govor bio detektovan (u sekundama)\n",
    "global flag_record #flag se aktivira ako su usta otvorena ili ako je proslo < 0.5 sekundi od zatvaranja usta(verovatno izgovaranje m ili p slova)\n",
    "global prev_flag_record #da bi detektovali prelaz flag_record sa 1 na 0 i znamo da je gotova sekvenca pricanja i mozemo da obradimo podatke(funkcija procces_webcam)\n",
    "\n",
    "flag_record = False \n",
    "is_talking = False\n",
    "last_talking_time = 0\n",
    "prev_flag_record = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf196244-dd2b-4f76-b38c-05859b2ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcija koja procesuira frejm kada je detektovan govor na web-kameri \n",
    "#i vraća tensor sa frejmovima govora, pripremljen za predikciju u neuronskoj mreži\n",
    "\n",
    "def process_frame(detector, predictor, frame, width: int = 64, height: int = 64) -> List[float]: \n",
    "    \n",
    "    global flag_record\n",
    "    if flag_record:\n",
    "        frames = []\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray_frame)\n",
    "        lip_coords = None\n",
    "    \n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray_frame, face)\n",
    "            \n",
    "            lip_left = landmarks.part(48).x\n",
    "            lip_right = landmarks.part(54).x\n",
    "            lip_top = min(landmarks.part(50).y, landmarks.part(51).y)\n",
    "            lip_bottom = max(landmarks.part(58).y, landmarks.part(59).y)\n",
    "    \n",
    "            lip_frame = frame[lip_top:lip_bottom, lip_left:lip_right]\n",
    "            lip_frame_resized = cv2.resize(lip_frame, (width, height))  # Define `width` and `height`\n",
    "            lip_frame_gray = cv2.cvtColor(lip_frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "            frames.append(lip_frame_gray)\n",
    "            \n",
    "            # Update lip_coords with actual coordinates if needed\n",
    "            lip_coords = (lip_left, lip_right, lip_top, lip_bottom)\n",
    "    \n",
    "        if frames:\n",
    "            frames = tf.convert_to_tensor(frames, dtype=tf.float32)\n",
    "            mean = tf.reduce_mean(frames)\n",
    "            std = tf.math.reduce_std(frames)\n",
    "            frames = tf.cast((frames - mean), tf.float32) / std\n",
    "            return frames, lip_coords\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435f9a05-a9a6-4e3c-96c1-54babd768082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizuelna reprezentacija na web-kameri koja pokazuje da je algoritam uspešno detektovao lice i izdvojio regiju usta\n",
    "\n",
    "def process_frame_overlay(detector, predictor,  frame, width: int = 64, height: int = 64) -> np.ndarray: \n",
    "    \n",
    "    global flag_record\n",
    "    global is_talking, last_talking_time\n",
    "    current_time = time.time()\n",
    "        \n",
    "    frames = []\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray_frame)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        cv2.putText(frame, \"No Face detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    else:\n",
    "        \n",
    "        for face in faces:\n",
    "            \n",
    "            landmarks = predictor(gray_frame, face)\n",
    "            \n",
    "            lip_left = landmarks.part(48).x\n",
    "            lip_right = landmarks.part(54).x\n",
    "            lip_top = min(landmarks.part(50).y, landmarks.part(51).y)\n",
    "            lip_bottom = max(landmarks.part(58).y, landmarks.part(59).y)\n",
    "    \n",
    "            #print(f\"Lip coordinates: left={lip_left}, right={lip_right}, top={lip_top}, bottom={lip_bottom}\")\n",
    "            mouth_top = (landmarks.part(51).x, landmarks.part(51).y)\n",
    "            mouth_bottom = (landmarks.part(57).x, landmarks.part(57).y)\n",
    "            lip_distance = math.hypot(mouth_bottom[0] - mouth_top[0], mouth_bottom[1] - mouth_top[1])\n",
    "            \n",
    "            lip_frame = frame[lip_top:lip_bottom, lip_left:lip_right]\n",
    "            lip_frame_resized = cv2.resize(lip_frame, (width, height))\n",
    "            lip_frame_gray = cv2.cvtColor(lip_frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "                 \n",
    "            frames.append(lip_frame_gray)\n",
    "            #print(lip_distance)\n",
    "            if lip_distance >= 19.5:\n",
    "                \n",
    "                flag_record = True\n",
    "                is_talking = True\n",
    "               \n",
    "                last_talking_time = current_time\n",
    "                \n",
    "                cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                for n in range(48, 61):\n",
    "                    x = landmarks.part(n).x\n",
    "                    y = landmarks.part(n).y\n",
    "                    cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                if is_talking and (current_time - last_talking_time >= 0.5):\n",
    "\n",
    "                    flag_record = False\n",
    "                    is_talking = False\n",
    "                    \n",
    "                    cv2.putText(frame, \"Silent\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                    for n in range(48, 61):\n",
    "                        x = landmarks.part(n).x\n",
    "                        y = landmarks.part(n).y\n",
    "                        cv2.circle(img=frame, center=(x, y), radius=3, color=(255, 0, 0), thickness=-1)\n",
    "                else:\n",
    "                    if is_talking:\n",
    "\n",
    "                        flag_record = True\n",
    "                        \n",
    "                        cv2.putText(frame, \"Talking\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        for n in range(48, 61):\n",
    "                            x = landmarks.part(n).x\n",
    "                            y = landmarks.part(n).y\n",
    "                            cv2.circle(img=frame, center=(x, y), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "                    else:\n",
    "\n",
    "                        flag_record = False\n",
    "                        \n",
    "                        cv2.putText(frame, \"Silent\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "                        for n in range(48, 61):\n",
    "                            x = landmarks.part(n).x\n",
    "                            y = landmarks.part(n).y\n",
    "                            cv2.circle(img=frame, center=(x, y), radius=3, color=(255, 0, 0), thickness=-1)\n",
    "   \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a395e4c9-9a38-4afb-87e8-c9b7bb255fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcija koja snima deo u kojem je detektovan govor i čuva ga kao GIF, \n",
    "#pružajući vizuelnu reprezentaciju podataka koji će biti korišćeni u neuronskoj mreži.\n",
    "\n",
    "def save_talking_as_gif(collected_frames):\n",
    "    \n",
    "        frames_array = np.array(collected_frames)\n",
    "        frames_array_uint8 = np.uint8(frames_array * 255)\n",
    "        frames_array_uint8_squeezed = np.squeeze(frames_array_uint8)\n",
    "        imageio.mimsave('./animation1.gif', frames_array_uint8_squeezed, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16faace0-d9c3-46ae-8b92-17a404720559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while loop\n",
    "\n",
    "def process_webcam():\n",
    "    \n",
    "    global prev_flag_record\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    collected_frames = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "\n",
    "        flipped_frame = cv2.flip(frame, 1)\n",
    "        lip_frames, _ = process_frame(detector, predictor, flipped_frame) \n",
    "        processed_frame_overlay = process_frame_overlay(detector, predictor, flipped_frame) \n",
    "        \n",
    "        if lip_frames is not None: \n",
    "            collected_frames.extend(lip_frames)\n",
    "        \n",
    "        if prev_flag_record == 1 and flag_record == 0:  #detektujemo prelaz sa 1 na 0 (tj prestanak govora i ponovo cutanje i znamo da mozemo da obradimo taj snimak)\n",
    "                if collected_frames:\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices(collected_frames)\n",
    "                    save_talking_as_gif(collected_frames) #vrati animation1.gif gde je snimljena sekvenca govora\n",
    "                    print('ovde ide predikcija teksta...')               \n",
    "                    collected_frames = [] #nakon predikcije teksta cistimo listu gde je bio govor za sledeci govor\n",
    "                    \n",
    "        prev_flag_record = flag_record \n",
    "        \n",
    "        cv2.imshow('Webcam Overlay', processed_frame_overlay)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd66e999-6dfb-4d16-b1df-07adb96d8f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n",
      "ovde ide predikcija teksta...\n"
     ]
    }
   ],
   "source": [
    "process_webcam()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5eec1-7bd9-4f59-9324-eaa1ba272239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51b499-c054-4e9e-9396-b7406d597cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
